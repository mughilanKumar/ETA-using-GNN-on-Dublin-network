{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "collapsed_sections": [
        "68IJtJYvGhss",
        "K3poL_zWVbhk",
        "nh_lQIVOWsQW",
        "AfXv1heBbfJt",
        "edueN8JmuOzX"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# INTRODUCTION"
      ],
      "metadata": {
        "id": "68IJtJYvGhss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Research Question**\n",
        "Can the Estimated Time of Arrival on a Dublin Road Network be accurately predicted using Graph Neural Networks ?\n",
        "\n",
        "**Aims**\n",
        "\n",
        "* Investigate the potential of GNNs to model and predict ETAs in transportation networks.\n",
        "* Analyze the strengths and limitations of each approach in capturing the temporal and spatial dynamics of transportation data.\n",
        "\n",
        "\n",
        "**Objectives**\n",
        "\n",
        "* Review and understand the foundational concepts and applications of GNNs from existing literature, especially in the context of ETA prediction.\n",
        "* Acquire and pre-process the dataset to make it suitable for graph-based neural network modelling.\n",
        "* Develop a GNN-based models which incorporate spatial and  temporal dynamics to predict ETAs\n",
        "* Identify appropriate performance metrics to assess the accuracy and efficiency of the model.\n",
        "* Compare the performance of the GNN models, highlighting the advantages and drawbacks of each approach.\n",
        "* Visualize and interpret the results, drawing insights about the factors impacting ETA predictions.\n"
      ],
      "metadata": {
        "id": "7iAG1rgpxsPp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DATA**\n",
        "\n",
        "Explanation of definitions.\n",
        "\n",
        "**Route** A group of two or more Control Sites usually along a common section of roadway. The Route is made up of one or more links.\n",
        "\n",
        "**Link** Two adjacent junctions with Control Sites and the corresponding section of intermediate route.\n",
        "\n",
        "**STT** Smoothed Travel Time.\n",
        "\n",
        "**AccSTT** Accumulated Smoothed Travel Time.\n",
        "\n",
        "**TCS** Traffic Control Site. This is the SCATS Site ID for the junction and it is unique city-wide.\n"
      ],
      "metadata": {
        "id": "vFfoIO2Ny5BM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Import"
      ],
      "metadata": {
        "id": "K3poL_zWVbhk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading \"Dublin Trips\" file as data."
      ],
      "metadata": {
        "id": "QPEzXeSLUHnu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQmWmYvN1Wr3"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries and reading the CSV file\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!wget -O \"/content/drive/My Drive/TRIPS/trips-1-day.csv\" \"https://data.smartdublin.ie/dataset/d083b9a8-bed7-444c-a387-d58318f31c5d/resource/3bf193dc-6029-42e7-987f-31ea5ae3c32f/download/trips-1-day.csv\"\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/My Drive/TRIPS/trips-1-day.csv\", error_bad_lines=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "896UiLb_VuEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "dQsq9u5nlCuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with any null values\n",
        "data = data.dropna()\n",
        "\n",
        "# Convert 'Timestamp' to datetime format\n",
        "data['timestamp'] = pd.to_datetime(data['Timestamp'], format='%Y%m%d-%H%M')\n",
        "\n",
        "# Reformat 'Timestamp' to 'DD-MM-YYYY HH:MM' format\n",
        "data['timestamp'] = data['timestamp'].dt.strftime('%d-%m-%Y %H:%M')\n",
        "\n",
        "data = data.loc[data['timestamp']<'19-11-2015 10:00']"
      ],
      "metadata": {
        "id": "AFn8T06fldfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Exploration and Analysis"
      ],
      "metadata": {
        "id": "nh_lQIVOWsQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for missing values in the dataset\n",
        "missing_values = data.isnull().sum()\n",
        "\n",
        "missing_values"
      ],
      "metadata": {
        "id": "E5v_hpjoWrYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The visualization tool, matplotlib, is incorporated to display data graphically.The first histogram showcases the distribution of \"STT\" values from the dataThe second histogram displays the distribution of \"AccSTT\" values from the data"
      ],
      "metadata": {
        "id": "b0W-b31ZXY4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting the distribution of STT and AccSTT values\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# STT distribution\n",
        "axes[0].hist(data[\"STT\"], bins=50, color='blue', edgecolor='black')\n",
        "axes[0].set_title('Distribution of STT Values')\n",
        "axes[0].set_xlabel('STT')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "\n",
        "# AccSTT distribution\n",
        "axes[1].hist(data[\"AccSTT\"], bins=50, color='green', edgecolor='black')\n",
        "axes[1].set_title('Distribution of AccSTT Values')\n",
        "axes[1].set_xlabel('AccSTT')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WBQonTJuWrNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of distinct values for the columns \"Route\", \"Link\", \"TCS1\", and \"TCS2\" in the data are computed.unique_counts provides a summary of how many unique values are present for each of these columns."
      ],
      "metadata": {
        "id": "Zz62iU7wYEgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting unique values for routes, links, and TCS values\n",
        "unique_routes = data[\"Route\"].nunique()\n",
        "unique_links = data[\"Link\"].nunique()\n",
        "unique_direction = data[\"Direction\"].nunique()\n",
        "unique_tcs1 = data[\"TCS1\"].nunique()\n",
        "unique_tcs2 = data[\"TCS2\"].nunique()\n",
        "\n",
        "unique_counts = {\n",
        "    \"Unique Routes\": unique_routes,\n",
        "    \"Unique Links\": unique_links,\n",
        "    \"unique direction\": unique_direction,\n",
        "    \"Unique TCS1\": unique_tcs1,\n",
        "    \"Unique TCS2\": unique_tcs2\n",
        "}\n",
        "\n",
        "unique_counts"
      ],
      "metadata": {
        "id": "TdQrXvK1WrLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ther are totaly 50 and in each route there are upto 30 links and each links have 2 directions , these links are the connection between TCS1 and TCS2"
      ],
      "metadata": {
        "id": "QWtEE92uZclG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = data"
      ],
      "metadata": {
        "id": "LqK5yZAbLX2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Visualisation (Nodes and Edges)"
      ],
      "metadata": {
        "id": "F1KZdcjgaWE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import folium\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# Parse the KML file\n",
        "!wget -O \"/content/drive/My Drive/TRIPS/routes.kml\" \"https://data.smartdublin.ie/dataset/d083b9a8-bed7-444c-a387-d58318f31c5d/resource/cd3f099c-e00a-49cc-8d02-ed2e10b8fc3e/download/routes.kml\"\n",
        "tree = ET.parse('/content/drive/My Drive/TRIPS/routes.kml')\n",
        "root = tree.getroot()\n",
        "\n",
        "# Extracting namespace for KML\n",
        "namespace = {\"kml\": root.tag.split('}')[0].strip('{')}\n",
        "\n",
        "# Extracting route details from placemarks\n",
        "routes_data = []\n",
        "for placemark in root.findall(\".//kml:Placemark\", namespace):\n",
        "    route_id = None\n",
        "    extended_data_element = placemark.find(\"kml:ExtendedData\", namespace)\n",
        "    if extended_data_element is not None:\n",
        "        for data in extended_data_element:\n",
        "            for value in data:\n",
        "                route_id = value.text\n",
        "\n",
        "    line_string_element = placemark.find(\"kml:LineString\", namespace)\n",
        "    if line_string_element is not None:\n",
        "        coordinates = line_string_element.find(\"kml:coordinates\", namespace).text.strip().split(\" \")\n",
        "        for coord_pair in coordinates:\n",
        "            longitude, latitude = coord_pair.split(\",\")[:2]\n",
        "            routes_data.append({\"RouteID\": route_id, \"Longitude\": float(longitude), \"Latitude\": float(latitude)})\n",
        "\n",
        "# Convert to DataFrame\n",
        "routes_df = pd.DataFrame(routes_data)\n",
        "\n",
        "# Load the sample dataset\n",
        "!wget -O \"/content/drive/My Drive/TRIPS/trips.csv\" \"https://opendata.dublincity.ie/TrafficOpenData/CP_TR/trips.csv\"\n",
        "data = pd.read_csv('/content/drive/My Drive/TRIPS/trips.csv')\n",
        "\n",
        "# Extract unique combinations of TCS1, TCS2, # Route, Link, and Direction\n",
        "unique_combinations = data[[\"TCS1\", \"TCS2\", \"# Route\", \"Link\", \"Direction\"]].drop_duplicates()\n",
        "unique_tcs = set(unique_combinations[\"TCS1\"]).union(set(unique_combinations[\"TCS2\"]))\n",
        "\n",
        "# Filter coordinates dataframe for unique TCS values\n",
        "filtered_routes_df = routes_df[routes_df[\"RouteID\"].isin(map(str, unique_tcs))]\n",
        "\n",
        "# Create a base map centered around Dublin City\n",
        "m = folium.Map(location=[53.349805, -6.26031], zoom_start=13)\n",
        "\n",
        "# Add nodes to the map\n",
        "for _, row in filtered_routes_df.iterrows():\n",
        "    folium.CircleMarker(\n",
        "        location=[row[\"Latitude\"], row[\"Longitude\"]],\n",
        "        radius=2,\n",
        "        color=\"blue\",\n",
        "        fill=True,\n",
        "        fill_color=\"blue\"\n",
        "    ).add_to(m)\n",
        "\n",
        "# Add edges to the map based on unique combinations\n",
        "for _, row in unique_combinations.iterrows():\n",
        "    start_coords = filtered_routes_df[filtered_routes_df[\"RouteID\"] == str(row[\"TCS1\"])][[\"Latitude\", \"Longitude\"]].values\n",
        "    end_coords = filtered_routes_df[filtered_routes_df[\"RouteID\"] == str(row[\"TCS2\"])][[\"Latitude\", \"Longitude\"]].values\n",
        "    if len(start_coords) > 0 and len(end_coords) > 0:\n",
        "        folium.PolyLine([start_coords[0], end_coords[0]], color=\"black\", weight=0.5).add_to(m)\n",
        "\n",
        "# Display the map\n",
        "m\n"
      ],
      "metadata": {
        "id": "T0lO52oAWrIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Blue dots shows the TCS and the line represent the links between the TCS"
      ],
      "metadata": {
        "id": "K4-9aL9TcMrR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Construction"
      ],
      "metadata": {
        "id": "AfXv1heBbfJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "here we are taking a different approach on preprocessing, as a graph usually the TCS - Traffic Control Site will selected as Nodes and the link between TCS will the edges and the Node feature will be Predicted For *T+1* time using the *T* Node feature.\n",
        "\n",
        "In our dataset the STT- Short Travel Time is the Y value that needs to be predicted and since the STT is the travel time between two TCS it will be an edge feature, so in our approach, we have used a method called **line graph transformation**. we have used the combination of **TCS1 and TCS2** as **NODE** , the **EDGES** between these Nodes are the common TCS the Nodes Shares\n",
        "\n",
        "line graph transformation, nodes represent edges of the original graph, and two nodes in the line graph are connected if their corresponding edges in the original graph share a node.The concept of a line graph is rooted in graph theory and has been used in various applications to study properties or relationships that are more naturally expressed between edges rather than nodes of a graph."
      ],
      "metadata": {
        "id": "GsXTr7mcfiXP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "creating a dataframe which holds unique Timestamp and node with the STT for each direction"
      ],
      "metadata": {
        "id": "9H_GNkqyomer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting TCS1 and TCS2 to string and then concatenating\n",
        "\n",
        "df['node'] = np.where(df['Direction'] == 1,\n",
        "                      df['TCS1'].astype(str) + '-' + df['TCS2'].astype(str),\n",
        "                      df['TCS2'].astype(str) + '-' + df['TCS1'].astype(str))\n",
        "\n",
        "\n",
        "# Pivoting table to get STT values for both directions as separate columns\n",
        "pivot_df = df.pivot_table(index=['Timestamp', 'node'], columns='Direction', values='STT', aggfunc='first').reset_index()\n",
        "\n",
        "# Filling missing values with 0 since there might be few links which will have only one direction\n",
        "pivot_df.fillna(0, inplace=True)\n",
        "\n",
        "# Displaying the transformed dataframe\n",
        "pivot_df.head()\n"
      ],
      "metadata": {
        "id": "LFB5NLeO1j_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing torch-geomentric for graph construction\n",
        "\n",
        "!pip install torch-geometric"
      ],
      "metadata": {
        "id": "MYP2MPtR25n_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to create graph each timestep with Y-value(STT of T+1)"
      ],
      "metadata": {
        "id": "FlsCV8W2nEb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import itertools\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "def create_graph_for_timestamp_with_target(df, ts):\n",
        "    timestamp_df = df[df['Timestamp'] == ts]\n",
        "\n",
        "    nodes = timestamp_df['node'].unique().tolist()\n",
        "    node_index = {node: i for i, node in enumerate(nodes)}\n",
        "\n",
        "    edge_index = torch.tensor([[node_index[src], node_index[dst]] for src, dst in itertools.product(nodes, nodes)], dtype=torch.long).t().contiguous()\n",
        "\n",
        "    x = torch.tensor(timestamp_df.sort_values(by='node')[[1, 2]].values, dtype=torch.float)\n",
        "\n",
        "    next_ts_idx = df['Timestamp'].searchsorted(ts, side='right')\n",
        "    next_ts = df['Timestamp'].iloc[next_ts_idx] if next_ts_idx < len(df['Timestamp']) else None\n",
        "\n",
        "    if next_ts is None:\n",
        "        y = torch.zeros_like(x)\n",
        "    else:\n",
        "        next_ts_df = df[df['Timestamp'] == next_ts].sort_values(by='node')\n",
        "        y = torch.tensor(next_ts_df[[1, 2]].values, dtype=torch.float)\n",
        "\n",
        "    data = Data(x=x, edge_index=edge_index, y=y)\n",
        "    data.timestamp = ts\n",
        "    return data"
      ],
      "metadata": {
        "id": "gZkMfPrJ2kPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a graph for each timestamp using the function\n",
        "\n",
        "timestamps = pivot_df['Timestamp'].unique()\n",
        "graphs_with_target = [create_graph_for_timestamp_with_target(pivot_df, ts) for ts in timestamps]"
      ],
      "metadata": {
        "id": "TM4JYyHd3AOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(graphs_with_target))"
      ],
      "metadata": {
        "id": "ZTWnnLfu__Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(graphs_with_target)"
      ],
      "metadata": {
        "id": "85zotVi7uLjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Splitting and Normalization"
      ],
      "metadata": {
        "id": "edueN8JmuOzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting data"
      ],
      "metadata": {
        "id": "1m9RQh-WuiJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determing the split point\n",
        "split_idx = int(0.8 * len(timestamps))\n",
        "\n",
        "# Spliting the data based on the determined index\n",
        "train_timestamps = timestamps[:split_idx]\n",
        "test_timestamps = timestamps[split_idx:]\n",
        "\n",
        "train_dataset = [graph for graph in graphs_with_target if graph.timestamp in train_timestamps]\n",
        "test_dataset = [graph for graph in graphs_with_target if graph.timestamp in test_timestamps]"
      ],
      "metadata": {
        "id": "hf-9ImfquLZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalization"
      ],
      "metadata": {
        "id": "7iAl3TJbg2Lk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing mean and standard deviation of target values in the training set\n",
        "train_targets = [data.y for data in train_dataset]\n",
        "all_train_targets = torch.cat(train_targets, dim=0)\n",
        "mean_target = all_train_targets.mean(dim=0)\n",
        "std_target = all_train_targets.std(dim=0)\n",
        "\n",
        "# Normalize target values in training and testing datasets\n",
        "for data in train_dataset:\n",
        "    data.y = (data.y - mean_target) / std_target\n",
        "\n",
        "for data in test_dataset:\n",
        "    data.y = (data.y - mean_target) / std_target\n",
        "\n",
        "mean_target, std_target"
      ],
      "metadata": {
        "id": "SlntVQqjgiMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Loader**"
      ],
      "metadata": {
        "id": "nvK6SNkQujiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating data loaders\n",
        "from torch_geometric.data import DataLoader\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n"
      ],
      "metadata": {
        "id": "RL7Zz_QPuLWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get one batch of data from the train_loader\n",
        "sample_batch = next(iter(train_loader))\n",
        "\n",
        "# Print the shape of node features and edge indices\n",
        "print(\"Node features shape:\", sample_batch.x.shape)\n",
        "print(\"Edge index shape:\", sample_batch.edge_index.shape)\n",
        "\n",
        "# If dataset contains target values\n",
        "if hasattr(sample_batch, 'y'):\n",
        "    print(\"Target/Label shape:\", sample_batch.y.shape)"
      ],
      "metadata": {
        "id": "drlABV-fABAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Temporal Graph Convolution Recurrent Network"
      ],
      "metadata": {
        "id": "OI5cMypLb4CP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TGC-RN Model"
      ],
      "metadata": {
        "id": "wHb334JBb2es"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class TGCRN_STTPredictor(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
        "        super(TGCRN_STTPredictor, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim1)\n",
        "        self.recurrent_layer = nn.GRU(hidden_dim1, hidden_dim1, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim1, output_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        # First Graph Convolution Layer\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Using the batch vector to determine the start and end of each graph's nodes in the batch\n",
        "        x_list = [x[batch == i] for i in range(batch.max() + 1)]\n",
        "        x_packed = torch.nn.utils.rnn.pack_sequence(x_list, enforce_sorted=False)\n",
        "\n",
        "        # Recurrent Layer\n",
        "        x, _ = self.recurrent_layer(x_packed)\n",
        "\n",
        "        # Unpack the output from the GRU to restore the original sequence lengths\n",
        "        x, _ = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
        "\n",
        "        # Concatenate the outputs for all graphs in the batch to match original number of nodes\n",
        "        x = torch.cat([x[i][:len(x_list[i])] for i in range(x.size(0))], dim=0)\n",
        "\n",
        "        # Apply ReLU activation to the recurrent output\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Fully Connected Layer to produce the output\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "input_dim = 2  # Because we have STT values for two directions as input\n",
        "hidden_dim1 = 64\n",
        "hidden_dim2 = 32\n",
        "output_dim = 2  # Predict STT for both directions\n",
        "\n",
        "TGCRN_model = TGCRN_STTPredictor(input_dim, hidden_dim1, hidden_dim2, output_dim)\n"
      ],
      "metadata": {
        "id": "2Cbb04n1Df9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TGC-RN Training"
      ],
      "metadata": {
        "id": "L6VwzsZxAs5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "num_epochs = 100\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = torch.optim.Adam(TGCRN_model.parameters(), lr=learning_rate)\n",
        "criterion = nn.MSELoss()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "TGCRN_model = TGCRN_model.to(device)\n",
        "\n",
        "TGCRN_loss_values = []\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(num_epochs):\n",
        "    TGCRN_model.train()  # Set the model to training mode\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)  # Move the batch data to the device\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = TGCRN_model(batch)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(outputs, batch.y)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Print the average loss for this epoch\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}, Loss: {average_loss:.4f}\")\n",
        "    TGCRN_loss_values.append(average_loss)"
      ],
      "metadata": {
        "id": "H0ad6dQoGxZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Exclude the first loss value\n",
        "epochs = range(2, len(TGCRN_loss_values) + 1)\n",
        "losses = TGCRN_loss_values[1:]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(epochs, losses, label=\"Training Loss\", marker='o')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss Over Epochs for TGC-RN Model\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w1CXyYDgnlcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TGC-RN Evaluation"
      ],
      "metadata": {
        "id": "lvBFvwocAxye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TGCRN_model.eval()\n",
        "test_loss_TGCRN = 0\n",
        "all_predictions_TGCRN = []\n",
        "all_true_values_TGCRN = []\n",
        "\n",
        "for data in test_loader:\n",
        "    data = data.to(device)\n",
        "    with torch.no_grad():\n",
        "        predictions = TGCRN_model(data)\n",
        "\n",
        "\n",
        "    loss = criterion(predictions, data.y)\n",
        "    test_loss_TGCRN += loss.item()\n",
        "\n",
        "    all_predictions_TGCRN.append(predictions.cpu().numpy())\n",
        "    all_true_values_TGCRN.append(data.y.cpu().numpy())\n",
        "\n",
        "print(f\"Test Loss: {test_loss_TGCRN:.4f}\")\n"
      ],
      "metadata": {
        "id": "3nXq_kebHOPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Flatten the lists\n",
        "flattened_predictions_TGCRN = np.concatenate(all_predictions_TGCRN, axis=0)\n",
        "flattened_true_values_TGCRN = np.concatenate(all_true_values_TGCRN, axis=0)\n",
        "\n",
        "# Calculate metrics\n",
        "mae_TGCRN = mean_absolute_error(flattened_true_values_TGCRN, flattened_predictions_TGCRN)\n",
        "mse_TGCRN = mean_squared_error(flattened_true_values_TGCRN, flattened_predictions_TGCRN)\n",
        "rmse_TGCRN = np.sqrt(mse_TGCRN)\n",
        "r2_TGCRN = r2_score(flattened_true_values_TGCRN, flattened_predictions_TGCRN)\n",
        "\n",
        "# Create a DataFrame\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Metrics': ['Test Loss', 'MAE', 'MSE', 'RMSE', 'R2'],\n",
        "    'TGC-RN': [test_loss_TGCRN, mae_TGCRN, mse_TGCRN, rmse_TGCRN, r2_TGCRN] \\\n",
        "})\n",
        "\n",
        "metrics_df"
      ],
      "metadata": {
        "id": "LZP22TARo7bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Specify the indices of the timestamps you want to plot\n",
        "timestamps_to_plot = [0]  # Adjust this list as per needs\n",
        "\n",
        "figures = []\n",
        "\n",
        "# Plotting\n",
        "for timestamp_index in timestamps_to_plot:\n",
        "    if timestamp_index < len(all_true_values_TGCRN):\n",
        "        fig1 = plt.figure(figsize=(15, 5))\n",
        "\n",
        "        # Direction 1\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(all_true_values_TGCRN[timestamp_index][:50, 0], label=\"True Values\", marker='o')\n",
        "        plt.plot(all_predictions_TGCRN[timestamp_index][:50, 0], label=\"Predictions\", marker='x')\n",
        "        plt.legend()\n",
        "        plt.title(f\"TGC-RN Model Prediction for Direction 1\")\n",
        "        plt.xlabel(\"Time-step\")  # x-axis label\n",
        "        plt.ylabel(\"STT\")  # y-axis label\n",
        "\n",
        "        # Direction 2\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(all_true_values_TGCRN[timestamp_index][:50, 1], label=\"True Values\", marker='o')\n",
        "        plt.plot(all_predictions_TGCRN[timestamp_index][:50, 1], label=\"Predictions\", marker='x')\n",
        "        plt.legend()\n",
        "        plt.title(f\"TGC-RN Model Prediction for Direction 2\")\n",
        "        plt.xlabel(\"Time-step\")  # x-axis label\n",
        "        plt.ylabel(\"STT\")  # y-axis label\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        figures.append(fig1)\n"
      ],
      "metadata": {
        "id": "E0WOuCYhk_DY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Temporal Graph Chebyshev Convolution Network"
      ],
      "metadata": {
        "id": "Emk_PYFcwyyv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TGCCN Model"
      ],
      "metadata": {
        "id": "hQo_wucgQvZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import ChebConv\n",
        "\n",
        "class TGCCN_STTPredictor(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
        "        super(TGCCN_STTPredictor, self).__init__()\n",
        "        self.tgcn1 = ChebConv(input_dim, hidden_dim1, K=2)\n",
        "        self.recurrent_layer = torch.nn.LSTM(hidden_dim1, hidden_dim2, batch_first=True)\n",
        "        self.fc = torch.nn.Linear(hidden_dim2, output_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        # First Graph Convolution Layer\n",
        "        x = self.tgcn1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Recurrent Layer to capture temporal patterns\n",
        "        x, _ = self.recurrent_layer(x.unsqueeze(0))\n",
        "        x = x.squeeze(0)\n",
        "\n",
        "        # Apply ReLU activation to the LSTM output\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Fully Connected Layer to produce the output\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "input_dim = 2  # Because we have STT values for two directions as input\n",
        "hidden_dim1 = 64\n",
        "hidden_dim2 = 32\n",
        "output_dim = 2  # Predict STT for both directions\n",
        "\n",
        "TGCCN_model = TGCCN_STTPredictor(input_dim, hidden_dim1, hidden_dim2, output_dim)\n"
      ],
      "metadata": {
        "id": "kHl34u16ABrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TGCCN Training"
      ],
      "metadata": {
        "id": "me-yS3WKAWOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 0.01\n",
        "epochs = 100\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Use the Mean Squared Error Loss\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "# Use the Adam optimizer\n",
        "optimizer = torch.optim.Adam(TGCCN_model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Move model to the appropriate device\n",
        "TGCCN_model = TGCCN_model.to(device)\n",
        "\n",
        "TGCCN_loss_values = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    TGCCN_model.train()\n",
        "    total_loss = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = TGCCN_model(data)\n",
        "        loss = criterion(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n",
        "    TGCCN_loss_values.append(total_loss)\n"
      ],
      "metadata": {
        "id": "7b0LURz2vZ8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Exclude the first loss value\n",
        "epochs = range(1, len(TGCCN_loss_values) + 1)\n",
        "losses = TGCCN_loss_values[0:]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(epochs, losses, label=\"Training Loss\", marker='o')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"TGCCN Model Training Loss Over Epochs\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gIgQTuRGXH54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# loss from 5th epoch\n",
        "epochs = range(85, len(TGCCN_loss_values) + 1)\n",
        "losses = TGCCN_loss_values[84:]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(epochs, losses, label=\"Training Loss\", marker='o')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"TGCCN Model Training Loss Over Epochs\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZOEnEf5-E2n4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TGCCN Evaluation"
      ],
      "metadata": {
        "id": "SnozXDfhAfVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TGCCN_model.eval()\n",
        "test_loss_TGCCN = 0\n",
        "all_predictions_TGCCN = []\n",
        "all_true_values_TGCCN = []\n",
        "\n",
        "for data in test_loader:\n",
        "    data = data.to(device)\n",
        "    with torch.no_grad():\n",
        "        predictions = TGCCN_model(data)\n",
        "\n",
        "    # Check for shape mismatch and skip if they don't match\n",
        "    if predictions.shape != data.y.shape:\n",
        "        continue\n",
        "\n",
        "    loss = criterion(predictions, data.y)\n",
        "    test_loss_TGCCN += loss.item()\n",
        "\n",
        "    all_predictions_TGCCN.append(predictions.cpu().numpy())\n",
        "    all_true_values_TGCCN.append(data.y.cpu().numpy())\n",
        "\n",
        "print(f\"Test Loss: {test_loss_TGCCN:.4f}\")\n"
      ],
      "metadata": {
        "id": "uKFy2C3394BV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Flatten the lists\n",
        "flattened_predictions_TGCCN = np.concatenate(all_predictions_TGCCN, axis=0)\n",
        "flattened_true_values_TGCCN = np.concatenate(all_true_values_TGCCN, axis=0)\n",
        "\n",
        "# Calculate metrics\n",
        "mae_TGCCN = mean_absolute_error(flattened_true_values_TGCCN, flattened_predictions_TGCCN)\n",
        "mse_TGCCN = mean_squared_error(flattened_true_values_TGCCN, flattened_predictions_TGCCN)\n",
        "rmse_TGCCN = np.sqrt(mse_TGCCN)\n",
        "r2_TGCCN = r2_score(flattened_true_values_TGCCN, flattened_predictions_TGCCN)\n",
        "\n",
        "metrics_df['TGCCN'] = [test_loss_TGCCN, mae_TGCCN, mse_TGCCN, rmse_TGCCN, r2_TGCCN]\n",
        "\n",
        "metrics_df[['Metrics','TGCCN']]"
      ],
      "metadata": {
        "id": "aw-Jzkz9Qaxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Specify the indices of the timestamps you want to plot\n",
        "timestamps_to_plot = [0]\n",
        "\n",
        "\n",
        "\n",
        "# Plotting\n",
        "for timestamp_index in timestamps_to_plot:\n",
        "    if timestamp_index < len(all_true_values_TGCCN):\n",
        "        fig2 = plt.figure(figsize=(15, 5))\n",
        "\n",
        "        # Direction 1\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(all_true_values_TGCCN[timestamp_index][:50, 0], label=\"True Values\", marker='o')\n",
        "        plt.plot(all_predictions_TGCCN[timestamp_index][:50, 0], label=\"Predictions\", marker='x')\n",
        "        plt.legend()\n",
        "        plt.title(f\"TGCCN Model Prediction for Direction 1\")\n",
        "        plt.xlabel(\"Time-step\")  # x-axis label\n",
        "        plt.ylabel(\"STT\")  # y-axis label\n",
        "\n",
        "\n",
        "        # Direction 2\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(all_true_values_TGCCN[timestamp_index][:50, 1], label=\"True Values\", marker='o')\n",
        "        plt.plot(all_predictions_TGCCN[timestamp_index][:50, 1], label=\"Predictions\", marker='x')\n",
        "        plt.legend()\n",
        "        plt.title(f\"TGCCN Model Prediction for Direction 2\")\n",
        "        plt.xlabel(\"Time-step\")  # x-axis label\n",
        "        plt.ylabel(\"STT\")  # y-axis label\n",
        "\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        figures.append(fig2)"
      ],
      "metadata": {
        "id": "XJdu5pdbRSvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Result"
      ],
      "metadata": {
        "id": "iIHQsOeDGWaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_df"
      ],
      "metadata": {
        "id": "JK0UH2T_3xp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "\n",
        "for fig in [figures[0], figures[1]]:\n",
        "    display(fig)"
      ],
      "metadata": {
        "id": "9TxiX_1s3vTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oh8iVBKDNYHh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}